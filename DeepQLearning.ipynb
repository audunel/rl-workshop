{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartpole er et problem som består i en platform som kan bevege seg horisontalt og en pendel festet til platformen. Spillet går ut på å balansere pendelen over platformen. Observasjonsrommet er posisjonen og hastigheten til platformen, samt vinkelen og vinkelhastigheten til pendelen. Handlingene er et dytt mot venstre eller høyre.\n",
    "\n",
    "For å lære en strategi for dette problemet kan man bruke Deep Q Learning. Dette er en metode hvor man setter opp et dypt nevralt nettverk hvor inputen er observasjonen og outputen er en handling. Bellmans ligning gir\n",
    "\n",
    "$Q(S_t, A_t,\\theta) \\leftarrow Q(S_t, A_t,\\theta) + \\alpha [R_t + \\gamma \\max_a Q(S_{t+1}, a,\\theta) - Q(S_t,A_t, \\theta)]$\n",
    "\n",
    "Hvor $\\theta$ er vektene til Q-nettverket. Dette kan oversettes til problemet å minimere tapsfunksjonen\n",
    "\n",
    "$L(\\theta) = |\\underline{R_t + \\gamma \\max_a Q(S_{t+1}, a,\\theta)} - Q(S_t,A_t,\\theta)|^2$\n",
    "\n",
    "Ettersom man her optimaliserer mot en target (den understrekede delen av tapsfunksjonen) som ikke er stasjonær, altså endrer seg etterhvert som man trener nettverket, kan vi ikke garantere at treningen konvergerer. For å få en mer stabil trening vil man gjerne bruke to nettverk, et target nettverk og et treningsnettverk, hvor vektene fra targetnettverket kun endrer seg etter et gitt antall iterasjoner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_shape = (1,) + # ?\n",
    "num_outputs = # ?\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "# Definer model\n",
    "model.add(Dense(env.action_space.n, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteket <code>keras-rl</code> kommer med innebygd støtte for Deep Q nettverk. Her setter vi opp en agent som gitt en model, et environment og en policy (metode for å velge ut handling. Her bruker vi en grådig policy som bare velger den handlingen med størst vekting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory()\n",
    "dqn = DQNAgent(model=model, nb_actions= , memory=memory, nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n",
    "\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "dqn.fit(env, nb_steps=5000, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Til slutt kan vi teste agenten vår på problemet ved å kjøre <code>dqn.test()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dersom du blir ferdig kan du prøve å trene opp en agent på et mer komplisert miljø og nettverk, for eksempel et CNN på miljøet <code>\"Breakout-v0\"</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartpole er et problem som består i en platform som kan bevege seg horisontalt og en pendel festet til platformen. Spillet går ut på å balansere pendelen over platformen. Observasjonsrommet er posisjonen og hastigheten til platformen, samt vinkelen og vinkelhastigheten til pendelen. Handlingene er et dytt mot venstre eller høyre.\n",
    "\n",
    "For å lære en strategi for dette problemet kan man bruke Deep Q Learning. Dette er en metode hvor man setter opp et dypt nevralt nettverk hvor inputen er observasjonen og outputen er en handling. Bellmans ligning gir\n",
    "\n",
    "$Q(S_t, A_t,\\theta) \\leftarrow Q(S_t, A_t,\\theta) + \\alpha [R_t + \\gamma \\max_a Q(S_{t+1}, a,\\theta) - Q(S_t,A_t, \\theta)]$\n",
    "\n",
    "Hvor $\\theta$ er vektene til Q-nettverket. Dette kan oversettes til problemet å minimere tapsfunksjonen\n",
    "\n",
    "$L(\\theta) = |\\underline{R_t + \\gamma \\underset{a}{\\max} Q(S_{t+1}, a,\\theta)} - Q(S_t,A_t,\\theta)|^2$\n",
    "\n",
    "Ettersom man her optimaliserer mot en target (den understrekede delen av tapsfunksjonen) som ikke er stasjonær, altså endrer seg etterhvert som man trener nettverket, kan vi ikke garantere at treningen konvergerer. For å få en mer stabil trening vil man gjerne bruke to nettverk, et target nettverk og et treningsnettverk, hvor vektene fra targetnettverket kun endrer seg etter et gitt antall iterasjoner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 14:56:53.972738 4563498432 deprecation_wrapper.py:119] From /Users/auduneltvik/.virtualenvs/rl-workshop/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0724 14:56:53.996760 4563498432 deprecation_wrapper.py:119] From /Users/auduneltvik/.virtualenvs/rl-workshop/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0724 14:56:54.013818 4563498432 deprecation_wrapper.py:119] From /Users/auduneltvik/.virtualenvs/rl-workshop/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_shape = (1,) + # ?\n",
    "num_outputs = # ?\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "# Definer model\n",
    "model.add(Dense(env.action_space.n, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteket <code>keras-rl</code> kommer med innebygd støtte for Deep Q nettverk. Her setter vi opp en agent som gitt en model, et environment og en policy (metode for å velge ut handling. Her bruker vi en grådig policy som bare velger den handlingen med størst vekting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0724 14:56:55.307780 4563498432 deprecation_wrapper.py:119] From /Users/auduneltvik/.virtualenvs/rl-workshop/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0724 14:56:55.308681 4563498432 deprecation_wrapper.py:119] From /Users/auduneltvik/.virtualenvs/rl-workshop/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0724 14:56:55.397023 4563498432 deprecation_wrapper.py:119] From /Users/auduneltvik/.virtualenvs/rl-workshop/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "   35/5000: episode: 1, duration: 0.059s, episode steps: 35, steps per second: 590, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.089 [-0.846, 0.325], loss: --, mean_squared_error: --, mean_q: --\n",
      "   85/5000: episode: 2, duration: 0.015s, episode steps: 50, steps per second: 3280, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.115 [-0.286, 0.821], loss: --, mean_squared_error: --, mean_q: --\n",
      "  128/5000: episode: 3, duration: 0.010s, episode steps: 43, steps per second: 4391, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.080 [-0.882, 0.308], loss: --, mean_squared_error: --, mean_q: --\n",
      "  243/5000: episode: 4, duration: 0.025s, episode steps: 115, steps per second: 4603, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.006 [-0.972, 0.415], loss: --, mean_squared_error: --, mean_q: --\n",
      "  419/5000: episode: 5, duration: 0.043s, episode steps: 176, steps per second: 4072, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.517, 1.154], loss: --, mean_squared_error: --, mean_q: --\n",
      "  455/5000: episode: 6, duration: 0.010s, episode steps: 36, steps per second: 3735, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.113 [-0.168, 0.717], loss: --, mean_squared_error: --, mean_q: --\n",
      "  503/5000: episode: 7, duration: 0.011s, episode steps: 48, steps per second: 4202, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.078 [-0.975, 0.274], loss: --, mean_squared_error: --, mean_q: --\n",
      "  549/5000: episode: 8, duration: 0.011s, episode steps: 46, steps per second: 4188, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.077 [-0.345, 0.748], loss: --, mean_squared_error: --, mean_q: --\n",
      "  613/5000: episode: 9, duration: 0.017s, episode steps: 64, steps per second: 3848, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.089 [-0.280, 0.812], loss: --, mean_squared_error: --, mean_q: --\n",
      "  648/5000: episode: 10, duration: 0.010s, episode steps: 35, steps per second: 3414, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.122 [-0.401, 0.899], loss: --, mean_squared_error: --, mean_q: --\n",
      "  689/5000: episode: 11, duration: 0.011s, episode steps: 41, steps per second: 3793, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.072 [-1.232, 0.393], loss: --, mean_squared_error: --, mean_q: --\n",
      "  738/5000: episode: 12, duration: 0.012s, episode steps: 49, steps per second: 4086, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.083 [-0.354, 0.777], loss: --, mean_squared_error: --, mean_q: --\n",
      "  778/5000: episode: 13, duration: 0.010s, episode steps: 40, steps per second: 3937, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-1.048, 0.236], loss: --, mean_squared_error: --, mean_q: --\n",
      "  828/5000: episode: 14, duration: 0.013s, episode steps: 50, steps per second: 3980, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.087 [-0.347, 0.703], loss: --, mean_squared_error: --, mean_q: --\n",
      "  888/5000: episode: 15, duration: 0.014s, episode steps: 60, steps per second: 4322, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.058 [-1.120, 0.356], loss: --, mean_squared_error: --, mean_q: --\n",
      "  917/5000: episode: 16, duration: 0.008s, episode steps: 29, steps per second: 3713, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.094 [-1.064, 0.478], loss: --, mean_squared_error: --, mean_q: --\n",
      "  955/5000: episode: 17, duration: 0.009s, episode steps: 38, steps per second: 4084, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.106 [-0.413, 0.785], loss: --, mean_squared_error: --, mean_q: --\n",
      " 1043/5000: episode: 18, duration: 0.460s, episode steps: 88, steps per second: 191, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.088 [-0.252, 0.844], loss: 0.478589, mean_squared_error: 0.587472, mean_q: 0.539560\n",
      " 1107/5000: episode: 19, duration: 0.122s, episode steps: 64, steps per second: 523, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.080 [-0.350, 0.729], loss: 0.453107, mean_squared_error: 0.550027, mean_q: 0.578911\n",
      " 1200/5000: episode: 20, duration: 0.172s, episode steps: 93, steps per second: 542, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.015 [-0.949, 0.412], loss: 0.413908, mean_squared_error: 0.496375, mean_q: 0.645589\n",
      " 1263/5000: episode: 21, duration: 0.115s, episode steps: 63, steps per second: 550, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.078 [-0.810, 0.178], loss: 0.374762, mean_squared_error: 0.444349, mean_q: 0.709361\n",
      " 1302/5000: episode: 22, duration: 0.072s, episode steps: 39, steps per second: 540, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.114 [-0.748, 0.362], loss: 0.353307, mean_squared_error: 0.418404, mean_q: 0.749821\n",
      " 1347/5000: episode: 23, duration: 0.083s, episode steps: 45, steps per second: 545, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.121 [-1.061, 0.248], loss: 0.341113, mean_squared_error: 0.402820, mean_q: 0.765921\n",
      " 1422/5000: episode: 24, duration: 0.133s, episode steps: 75, steps per second: 566, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.073 [-0.517, 0.723], loss: 0.321327, mean_squared_error: 0.378048, mean_q: 0.795246\n",
      " 1503/5000: episode: 25, duration: 0.156s, episode steps: 81, steps per second: 520, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.035 [-0.922, 0.408], loss: 0.309112, mean_squared_error: 0.363386, mean_q: 0.820623\n",
      " 1549/5000: episode: 26, duration: 0.086s, episode steps: 46, steps per second: 538, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.117 [-1.171, 0.236], loss: 0.303701, mean_squared_error: 0.359034, mean_q: 0.835990\n",
      " 1593/5000: episode: 27, duration: 0.082s, episode steps: 44, steps per second: 533, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.119 [-0.750, 0.500], loss: 0.291499, mean_squared_error: 0.342669, mean_q: 0.846596\n",
      " 1629/5000: episode: 28, duration: 0.070s, episode steps: 36, steps per second: 515, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.112 [-0.740, 0.206], loss: 0.290573, mean_squared_error: 0.342184, mean_q: 0.854244\n",
      " 1680/5000: episode: 29, duration: 0.095s, episode steps: 51, steps per second: 536, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.087 [-1.006, 0.280], loss: 0.282806, mean_squared_error: 0.331475, mean_q: 0.857637\n",
      " 1717/5000: episode: 30, duration: 0.069s, episode steps: 37, steps per second: 534, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.109 [-0.858, 0.369], loss: 0.275073, mean_squared_error: 0.321681, mean_q: 0.867145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1783/5000: episode: 31, duration: 0.129s, episode steps: 66, steps per second: 513, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.098 [-0.342, 0.801], loss: 0.276818, mean_squared_error: 0.325292, mean_q: 0.874250\n",
      " 1825/5000: episode: 32, duration: 0.162s, episode steps: 42, steps per second: 259, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.096 [-0.694, 0.174], loss: 0.265511, mean_squared_error: 0.309228, mean_q: 0.880464\n",
      " 1892/5000: episode: 33, duration: 0.200s, episode steps: 67, steps per second: 334, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.074 [-0.851, 0.600], loss: 0.264466, mean_squared_error: 0.308852, mean_q: 0.886799\n",
      " 1934/5000: episode: 34, duration: 0.103s, episode steps: 42, steps per second: 409, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.112 [-0.240, 0.867], loss: 0.265175, mean_squared_error: 0.311593, mean_q: 0.893611\n",
      " 1970/5000: episode: 35, duration: 0.075s, episode steps: 36, steps per second: 477, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.105 [-0.728, 0.204], loss: 0.262256, mean_squared_error: 0.307078, mean_q: 0.895211\n",
      " 2019/5000: episode: 36, duration: 0.103s, episode steps: 49, steps per second: 474, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.100 [-1.019, 0.408], loss: 0.263223, mean_squared_error: 0.309358, mean_q: 0.897830\n",
      " 2055/5000: episode: 37, duration: 0.069s, episode steps: 36, steps per second: 518, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.123 [-0.877, 0.172], loss: 0.257370, mean_squared_error: 0.300690, mean_q: 0.901491\n",
      " 2088/5000: episode: 38, duration: 0.063s, episode steps: 33, steps per second: 520, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.123 [-0.680, 0.157], loss: 0.270101, mean_squared_error: 0.322043, mean_q: 0.906476\n",
      " 2140/5000: episode: 39, duration: 0.103s, episode steps: 52, steps per second: 506, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.115 [-0.312, 0.835], loss: 0.266733, mean_squared_error: 0.315404, mean_q: 0.902449\n",
      " 2181/5000: episode: 40, duration: 0.078s, episode steps: 41, steps per second: 526, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.112 [-0.854, 0.365], loss: 0.257763, mean_squared_error: 0.302536, mean_q: 0.903775\n",
      " 2211/5000: episode: 41, duration: 0.058s, episode steps: 30, steps per second: 514, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.135 [-0.680, 0.354], loss: 0.243561, mean_squared_error: 0.283682, mean_q: 0.914519\n",
      " 2289/5000: episode: 42, duration: 0.152s, episode steps: 78, steps per second: 514, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.040 [-0.779, 0.362], loss: 0.246851, mean_squared_error: 0.289497, mean_q: 0.922111\n",
      " 2347/5000: episode: 43, duration: 0.110s, episode steps: 58, steps per second: 527, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.059 [-0.689, 0.365], loss: 0.246749, mean_squared_error: 0.289032, mean_q: 0.921155\n",
      " 2399/5000: episode: 44, duration: 0.102s, episode steps: 52, steps per second: 512, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.073 [-0.736, 0.237], loss: 0.246423, mean_squared_error: 0.288939, mean_q: 0.921140\n",
      " 2451/5000: episode: 45, duration: 0.095s, episode steps: 52, steps per second: 549, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.083 [-0.738, 0.390], loss: 0.232495, mean_squared_error: 0.268041, mean_q: 0.921566\n",
      " 2498/5000: episode: 46, duration: 0.089s, episode steps: 47, steps per second: 530, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.089 [-0.783, 0.348], loss: 0.249120, mean_squared_error: 0.293266, mean_q: 0.921375\n",
      " 2543/5000: episode: 47, duration: 0.086s, episode steps: 45, steps per second: 524, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.096 [-0.967, 0.198], loss: 0.242222, mean_squared_error: 0.282984, mean_q: 0.923560\n",
      " 2575/5000: episode: 48, duration: 0.059s, episode steps: 32, steps per second: 543, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.136 [-0.736, 0.388], loss: 0.249305, mean_squared_error: 0.292319, mean_q: 0.920050\n",
      " 2633/5000: episode: 49, duration: 0.109s, episode steps: 58, steps per second: 534, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.082 [-0.774, 0.449], loss: 0.246563, mean_squared_error: 0.289361, mean_q: 0.925265\n",
      " 2673/5000: episode: 50, duration: 0.076s, episode steps: 40, steps per second: 523, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.114 [-0.788, 0.264], loss: 0.240383, mean_squared_error: 0.281757, mean_q: 0.934680\n",
      " 2735/5000: episode: 51, duration: 0.116s, episode steps: 62, steps per second: 533, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.067 [-0.758, 0.190], loss: 0.245148, mean_squared_error: 0.288212, mean_q: 0.928107\n",
      " 2829/5000: episode: 52, duration: 0.256s, episode steps: 94, steps per second: 367, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.047 [-0.329, 0.719], loss: 0.240009, mean_squared_error: 0.281445, mean_q: 0.930785\n",
      " 2891/5000: episode: 53, duration: 0.133s, episode steps: 62, steps per second: 466, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.077 [-0.801, 0.269], loss: 0.226393, mean_squared_error: 0.262616, mean_q: 0.935686\n",
      " 2931/5000: episode: 54, duration: 0.087s, episode steps: 40, steps per second: 457, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.117 [-0.777, 0.203], loss: 0.237758, mean_squared_error: 0.278651, mean_q: 0.933200\n",
      " 2970/5000: episode: 55, duration: 0.086s, episode steps: 39, steps per second: 454, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.108 [-0.763, 0.360], loss: 0.227272, mean_squared_error: 0.263871, mean_q: 0.936719\n",
      " 3044/5000: episode: 56, duration: 0.160s, episode steps: 74, steps per second: 464, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.039 [-0.786, 0.379], loss: 0.231376, mean_squared_error: 0.269593, mean_q: 0.935808\n",
      " 3088/5000: episode: 57, duration: 0.111s, episode steps: 44, steps per second: 395, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.099 [-0.778, 0.420], loss: 0.242391, mean_squared_error: 0.285697, mean_q: 0.935975\n",
      " 3134/5000: episode: 58, duration: 0.101s, episode steps: 46, steps per second: 457, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.111 [-0.233, 0.858], loss: 0.222086, mean_squared_error: 0.257259, mean_q: 0.942850\n",
      " 3170/5000: episode: 59, duration: 0.091s, episode steps: 36, steps per second: 394, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.128 [-0.749, 0.413], loss: 0.241258, mean_squared_error: 0.283621, mean_q: 0.937455\n",
      " 3217/5000: episode: 60, duration: 0.105s, episode steps: 47, steps per second: 446, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.131 [-0.446, 0.880], loss: 0.231931, mean_squared_error: 0.271374, mean_q: 0.941084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3266/5000: episode: 61, duration: 0.123s, episode steps: 49, steps per second: 399, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.086 [-0.770, 0.264], loss: 0.221331, mean_squared_error: 0.256678, mean_q: 0.948590\n",
      " 3316/5000: episode: 62, duration: 0.113s, episode steps: 50, steps per second: 444, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.125 [-0.290, 0.814], loss: 0.233150, mean_squared_error: 0.273275, mean_q: 0.941777\n",
      " 3428/5000: episode: 63, duration: 0.277s, episode steps: 112, steps per second: 405, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.002 [-1.105, 0.424], loss: 0.223950, mean_squared_error: 0.259859, mean_q: 0.947021\n",
      " 3483/5000: episode: 64, duration: 0.154s, episode steps: 55, steps per second: 357, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.111 [-0.830, 0.561], loss: 0.223574, mean_squared_error: 0.260164, mean_q: 0.951383\n",
      " 3561/5000: episode: 65, duration: 0.168s, episode steps: 78, steps per second: 463, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.069 [-0.495, 0.777], loss: 0.215679, mean_squared_error: 0.249052, mean_q: 0.952230\n",
      " 3636/5000: episode: 66, duration: 0.140s, episode steps: 75, steps per second: 535, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.072 [-0.428, 0.913], loss: 0.230008, mean_squared_error: 0.269371, mean_q: 0.948124\n",
      " 3686/5000: episode: 67, duration: 0.097s, episode steps: 50, steps per second: 516, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.110 [-0.386, 0.847], loss: 0.223491, mean_squared_error: 0.259748, mean_q: 0.950426\n",
      " 3726/5000: episode: 68, duration: 0.074s, episode steps: 40, steps per second: 544, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.135 [-0.211, 0.841], loss: 0.232710, mean_squared_error: 0.273446, mean_q: 0.950438\n",
      " 3781/5000: episode: 69, duration: 0.105s, episode steps: 55, steps per second: 526, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.108 [-0.445, 0.869], loss: 0.229490, mean_squared_error: 0.269462, mean_q: 0.953750\n",
      " 3861/5000: episode: 70, duration: 0.164s, episode steps: 80, steps per second: 487, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.073 [-0.797, 0.379], loss: 0.225391, mean_squared_error: 0.263101, mean_q: 0.953845\n",
      " 3917/5000: episode: 71, duration: 0.106s, episode steps: 56, steps per second: 526, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.100 [-0.248, 1.024], loss: 0.226827, mean_squared_error: 0.264676, mean_q: 0.952186\n",
      " 3956/5000: episode: 72, duration: 0.075s, episode steps: 39, steps per second: 521, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.111 [-1.033, 0.368], loss: 0.221964, mean_squared_error: 0.258573, mean_q: 0.954582\n",
      " 4002/5000: episode: 73, duration: 0.105s, episode steps: 46, steps per second: 438, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.072 [-0.747, 0.358], loss: 0.231710, mean_squared_error: 0.272319, mean_q: 0.951162\n",
      " 4084/5000: episode: 74, duration: 0.314s, episode steps: 82, steps per second: 262, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.078 [-0.247, 0.827], loss: 0.222760, mean_squared_error: 0.259896, mean_q: 0.957559\n",
      " 4178/5000: episode: 75, duration: 0.228s, episode steps: 94, steps per second: 413, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.019 [-0.788, 0.543], loss: 0.213950, mean_squared_error: 0.247623, mean_q: 0.958211\n",
      " 4286/5000: episode: 76, duration: 0.199s, episode steps: 108, steps per second: 541, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.085 [-0.805, 0.658], loss: 0.214599, mean_squared_error: 0.248517, mean_q: 0.960581\n",
      " 4322/5000: episode: 77, duration: 0.068s, episode steps: 36, steps per second: 532, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.114 [-0.366, 1.111], loss: 0.220648, mean_squared_error: 0.257270, mean_q: 0.959960\n",
      " 4432/5000: episode: 78, duration: 0.204s, episode steps: 110, steps per second: 538, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.077 [-0.742, 0.769], loss: 0.212436, mean_squared_error: 0.244773, mean_q: 0.958184\n",
      " 4482/5000: episode: 79, duration: 0.098s, episode steps: 50, steps per second: 512, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.101 [-0.789, 0.335], loss: 0.217708, mean_squared_error: 0.252683, mean_q: 0.961163\n",
      " 4544/5000: episode: 80, duration: 0.120s, episode steps: 62, steps per second: 519, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.100 [-0.822, 0.426], loss: 0.218199, mean_squared_error: 0.254157, mean_q: 0.962914\n",
      " 4590/5000: episode: 81, duration: 0.090s, episode steps: 46, steps per second: 512, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.109 [-0.232, 0.793], loss: 0.212646, mean_squared_error: 0.245297, mean_q: 0.961581\n",
      " 4680/5000: episode: 82, duration: 0.171s, episode steps: 90, steps per second: 526, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.064 [-0.881, 0.390], loss: 0.212494, mean_squared_error: 0.245784, mean_q: 0.962417\n",
      " 4728/5000: episode: 83, duration: 0.110s, episode steps: 48, steps per second: 438, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.088 [-1.024, 0.261], loss: 0.211466, mean_squared_error: 0.244389, mean_q: 0.964746\n",
      " 4763/5000: episode: 84, duration: 0.069s, episode steps: 35, steps per second: 509, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.094 [-0.718, 0.362], loss: 0.218059, mean_squared_error: 0.254452, mean_q: 0.963100\n",
      " 4807/5000: episode: 85, duration: 0.084s, episode steps: 44, steps per second: 522, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.106 [-0.692, 0.178], loss: 0.222700, mean_squared_error: 0.261491, mean_q: 0.963047\n",
      " 4846/5000: episode: 86, duration: 0.074s, episode steps: 39, steps per second: 529, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.110 [-0.255, 0.817], loss: 0.219677, mean_squared_error: 0.257891, mean_q: 0.971060\n",
      " 4896/5000: episode: 87, duration: 0.095s, episode steps: 50, steps per second: 527, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.076 [-0.727, 0.187], loss: 0.209787, mean_squared_error: 0.242085, mean_q: 0.961556\n",
      " 4936/5000: episode: 88, duration: 0.078s, episode steps: 40, steps per second: 513, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.098 [-0.764, 0.211], loss: 0.204460, mean_squared_error: 0.234161, mean_q: 0.965978\n",
      " 4999/5000: episode: 89, duration: 0.119s, episode steps: 63, steps per second: 528, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.100 [-0.295, 0.846], loss: 0.217093, mean_squared_error: 0.253550, mean_q: 0.965432\n",
      "done, took 9.139 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12db367b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit = 50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=env.action_space.n, memory=memory, policy=policy)\n",
    "\n",
    "dqn.compile(Adam(), metrics=['mse'])\n",
    "\n",
    "dqn.fit(env, nb_steps=5000, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Til slutt kan vi teste agenten vår på problemet ved å kjøre <code>dqn.test()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dersom du blir ferdig kan du prøve å trene opp en agent på et mer komplisert miljø og nettverk, for eksempel et CNN på miljøet <code>\"Breakout-v0\"</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

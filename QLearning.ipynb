{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi må først sette opp et miljø. Gym kommer med et sett predefinerte miljøer, hvor 'FrozenLake-v0' er en av dem. Dette er et spill hvor en agent skal navigere en frossen insjø, som er representert med en 4x4 grid.\n",
    "\n",
    "Eksempel:\n",
    "\n",
    "SFFF<br>\n",
    "FHFH<br>\n",
    "FFFH<br>\n",
    "HFFG<br>\n",
    "\n",
    "S: start, trygg<br>\n",
    "F: frossen overflate, trygg<br>\n",
    "H: hull, ikke trygg<br>\n",
    "G: mål, hvor agenten ønsker å komme<br>\n",
    "\n",
    "En handling består i å bevege seg i en av fire retninger, men ettersom isen er glatt er det ikke garantert at man beveger seg i den retningen man ønsker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definer og sett opp miljøet\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "# env.action_space og env.obvservation_space definerer tilgjengelige handlinger og observasjoner\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "\n",
    "# vi kan visualisere et gitt miljø med env.render()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>env.action_space</code> er altså lik <code>Discrete(4)</code>. Dette vil si at den kan ta fire ulike diskret verdier, som representerer et forsøk på å bevege seg i en av de fire retningene. <code>env.observation_space</code> er lik <code>Discrete(16)</code>, altså 16 ulike verdier som representerer de 4x4=16 mulige posisjonene agenten kan ha.\n",
    "\n",
    "Det fins mange andre typer handlings- og tilstandsrom. Eksempelvis vil en handling i et handlingsrom lik <code>Box(4,)</code> være en array av fire tall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuler og visualiser fem tilfeldige handlinger\n",
    "env.reset()\n",
    "for _ in range(5):\n",
    "    env.render()\n",
    "    action = # Velg tilfeldig handling\n",
    "    observation, reward, done, info = # Utfør handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vi ser returnerer hvert steg fire verdier. De er som følger:\n",
    "\n",
    "<code>observation</code> En verdi fra observasjonsrommet<br>\n",
    "<code>reward</code> Et flyttall som representerer fortjenesten for agentens handling, i dette tilfellet 1.0 for målruten og 0.0 for alle andre<br>\n",
    "<code>done</code> En boolean som forteller oss om spillet er fullført (enten fordi målet er nådd eller agenten har falt ned i et hull)<br>\n",
    "<code>info</code> En dict med diverse debug-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"observation:\", observation)\n",
    "print(\"reward:\", reward)\n",
    "print(\"done:\", done)\n",
    "print(\"info:\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan lære en strategi for dette problemet ved bruk av Q-learning algoritmen.\n",
    "\n",
    "Epsilon-hyperparametrene under brukes for å kombinere Q-learning med simulated annealing: Vi velger\n",
    "en tilfeldig handling med en viss sannsynlighet, hvor sannsynligheten for å velge den optimale verdien gitt Q-verditabellen øker med tid.\n",
    "\n",
    "Nyttige funksjoner:\n",
    "\n",
    "$\\pi(s) = \\arg\\underset{a}{\\max} Q(s,a)$\n",
    "\n",
    "$Q^{new}(s_t, a_t) \\leftarrow  Q(s_t, a_t) + \\alpha*[r_t + \\gamma * \\underset{a}{\\max} Q(s_{t+1},a) - Q(s_t, a_t)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_q(env,\n",
    "            learning_rate = 0.7,\n",
    "            gamma = 0.9,\n",
    "            num_episodes = 20000,\n",
    "            max_steps = 100,\n",
    "            min_epsilon = 0.01,\n",
    "            max_epsilon = 1.0,\n",
    "            decay_rate = 0.005):\n",
    "    Q = # Tom tabell med størrelse [size(observation_space) x size(action_space)]\n",
    "    rewards = []\n",
    "\n",
    "    epsilon = max_epsilon\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        reward_total = 0.0\n",
    "        done = False\n",
    "        for i in range(max_steps):\n",
    "            tradeoff = np.random.uniform(0,1)\n",
    "\n",
    "            # Velg en handling basert på Q-tabell eller utforsk ny handling\n",
    "            if tradeoff > epsilon:\n",
    "                action = # Velg beste handling gitt nåværende Q-tabell\n",
    "            else:\n",
    "                action = # Velg en tilfeldig handling\n",
    "\n",
    "            state_new, reward, done, _ = # Utfør handling\n",
    "\n",
    "            # Oppdater Q-verdi for tilstand-handling-par\n",
    "            Q[state, action] = # Oppdateringsregel for Q-verdi\n",
    "            \n",
    "            # Oppdater total fortjeneste og tilstand\n",
    "            reward_total += reward\n",
    "            state = state_new\n",
    "\n",
    "            # Avslutt dersom spill fullføres\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(reward_total)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    final_reward = np.sum(rewards)/num_episodes\n",
    "    return Q, final_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, r = learn_q(env)\n",
    "print(\"Endelig resultat:\", r)\n",
    "print(\"Endelige Q-verdier:\", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Q-tabell\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = # Velg optimal handling gitt Q-tabell\n",
    "    state, reward, done, info = # Utfør handling\n",
    "\n",
    "if(state == 15):\n",
    "    print(\"Du klarte det! :D\")\n",
    "    env.render()\n",
    "else:\n",
    "    print(\"Du klarte det ikke :(\")\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eksperimenter gjerne med ulike hyperparametre for å se om du kan forbedre resultatet!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
